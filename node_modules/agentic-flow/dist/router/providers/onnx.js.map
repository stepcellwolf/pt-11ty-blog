{"version":3,"file":"onnx.js","sourceRoot":"","sources":["../../../src/router/providers/onnx.ts"],"names":[],"mappings":"AAAA;;;;;GAKG;AAYH,iDAAiD;AACjD,IAAI,GAAQ,CAAC;AACb,IAAI,YAAiB,CAAC;AAEtB,KAAK,UAAU,sBAAsB;IACnC,IAAI,CAAC,GAAG,EAAE,CAAC;QACT,IAAI,CAAC;YACH,MAAM,SAAS,GAAG,MAAM,MAAM,CAAC,kBAAyB,CAAC,CAAC;YAC1D,GAAG,GAAG,SAAS,CAAC;QAClB,CAAC;QAAC,OAAO,CAAC,EAAE,CAAC;YACX,MAAM,IAAI,KAAK,CAAC,mEAAmE,CAAC,CAAC;QACvF,CAAC;IACH,CAAC;IACD,IAAI,CAAC,YAAY,EAAE,CAAC;QAClB,IAAI,CAAC;YACH,MAAM,kBAAkB,GAAG,MAAM,MAAM,CAAC,sBAA6B,CAAC,CAAC;YACvE,YAAY,GAAG,kBAAkB,CAAC;YAClC,YAAY,CAAC,GAAG,CAAC,gBAAgB,GAAG,IAAI,CAAC;QAC3C,CAAC;QAAC,OAAO,CAAC,EAAE,CAAC;YACX,MAAM,IAAI,KAAK,CAAC,2EAA2E,CAAC,CAAC;QAC/F,CAAC;IACH,CAAC;AACH,CAAC;AAWD,MAAM,OAAO,YAAY;IACvB,IAAI,GAAG,MAAM,CAAC;IACd,IAAI,GAAG,QAAiB,CAAC;IACzB,iBAAiB,GAAG,IAAI,CAAC;IACzB,aAAa,GAAG,KAAK,CAAC;IACtB,WAAW,GAAG,KAAK,CAAC;IAEZ,OAAO,GAAQ,IAAI,CAAC;IACpB,SAAS,GAAQ,IAAI,CAAC;IACtB,MAAM,CAAa;IACnB,kBAAkB,GAAa,EAAE,CAAC;IAE1C,YAAY,SAAqB,EAAE;QACjC,IAAI,CAAC,MAAM,GAAG;YACZ,OAAO,EAAE,MAAM,CAAC,OAAO,IAAI,+BAA+B;YAC1D,SAAS,EAAE,MAAM,CAAC,SAAS,IAAI,GAAG;YAClC,WAAW,EAAE,MAAM,CAAC,WAAW,IAAI,GAAG;YACtC,GAAG,MAAM;SACV,CAAC;IACJ,CAAC;IAED;;OAEG;IACK,KAAK,CAAC,wBAAwB;QACpC,MAAM,SAAS,GAAa,EAAE,CAAC;QAE/B,2BAA2B;QAC3B,IAAI,CAAC;YACH,IAAI,OAAO,CAAC,QAAQ,KAAK,OAAO,EAAE,CAAC;gBACjC,SAAS,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;gBACvB,IAAI,CAAC,kBAAkB,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;YACvC,CAAC;QACH,CAAC;QAAC,OAAO,CAAC,EAAE,CAAC;YACX,qBAAqB;QACvB,CAAC;QAED,gCAAgC;QAChC,IAAI,CAAC;YACH,IAAI,OAAO,CAAC,QAAQ,KAAK,OAAO,EAAE,CAAC;gBACjC,SAAS,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;gBACtB,IAAI,CAAC,kBAAkB,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;YACtC,CAAC;QACH,CAAC;QAAC,OAAO,CAAC,EAAE,CAAC;YACX,yBAAyB;QAC3B,CAAC;QAED,yBAAyB;QACzB,SAAS,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;QACtB,IAAI,CAAC,kBAAkB,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;QAEpC,OAAO,CAAC,GAAG,CAAC,gCAAgC,IAAI,CAAC,kBAAkB,CAAC,IAAI,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;QAElF,OAAO,SAAS,CAAC;IACnB,CAAC;IAED;;OAEG;IACK,KAAK,CAAC,iBAAiB;QAC7B,IAAI,IAAI,CAAC,SAAS;YAAE,OAAO;QAE3B,IAAI,CAAC;YACH,MAAM,sBAAsB,EAAE,CAAC;YAE/B,OAAO,CAAC,GAAG,CAAC,0BAA0B,IAAI,CAAC,MAAM,CAAC,OAAO,EAAE,CAAC,CAAC;YAE7D,+CAA+C;YAC/C,IAAI,CAAC,SAAS,GAAG,MAAM,YAAY,CAAC,QAAQ,CAC1C,iBAAiB,EACjB,IAAI,CAAC,MAAM,CAAC,OAAO,EACnB;gBACE,SAAS,EAAE,IAAI,EAAE,kDAAkD;aACpE,CACF,CAAC;YAEF,OAAO,CAAC,GAAG,CAAC,kCAAkC,CAAC,CAAC;QAElD,CAAC;QAAC,OAAO,KAAK,EAAE,CAAC;YACf,MAAM,aAAa,GAAkB;gBACnC,IAAI,EAAE,eAAe;gBACrB,OAAO,EAAE,oCAAoC,KAAK,EAAE;gBACpD,QAAQ,EAAE,MAAM;gBAChB,SAAS,EAAE,KAAK;aACjB,CAAC;YACF,MAAM,aAAa,CAAC;QACtB,CAAC;IACH,CAAC;IAED;;OAEG;IACK,cAAc,CAAC,QAAmB;QACxC,iCAAiC;QACjC,IAAI,MAAM,GAAG,EAAE,CAAC;QAEhB,KAAK,MAAM,GAAG,IAAI,QAAQ,EAAE,CAAC;YAC3B,MAAM,OAAO,GAAG,OAAO,GAAG,CAAC,OAAO,KAAK,QAAQ;gBAC7C,CAAC,CAAC,GAAG,CAAC,OAAO;gBACb,CAAC,CAAC,GAAG,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,IAAI,KAAK,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;YAEnE,IAAI,GAAG,CAAC,IAAI,KAAK,MAAM,EAAE,CAAC;gBACxB,MAAM,IAAI,aAAa,OAAO,WAAW,CAAC;YAC5C,CAAC;iBAAM,IAAI,GAAG,CAAC,IAAI,KAAK,WAAW,EAAE,CAAC;gBACpC,MAAM,IAAI,kBAAkB,OAAO,WAAW,CAAC;YACjD,CAAC;iBAAM,IAAI,GAAG,CAAC,IAAI,KAAK,QAAQ,EAAE,CAAC;gBACjC,MAAM,IAAI,eAAe,OAAO,WAAW,CAAC;YAC9C,CAAC;QACH,CAAC;QAED,MAAM,IAAI,iBAAiB,CAAC;QAC5B,OAAO,MAAM,CAAC;IAChB,CAAC;IAED;;OAEG;IACH,KAAK,CAAC,IAAI,CAAC,MAAkB;QAC3B,MAAM,IAAI,CAAC,iBAAiB,EAAE,CAAC;QAE/B,MAAM,SAAS,GAAG,IAAI,CAAC,GAAG,EAAE,CAAC;QAC7B,MAAM,MAAM,GAAG,IAAI,CAAC,cAAc,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC;QAEpD,IAAI,CAAC;YACH,MAAM,MAAM,GAAG,MAAM,IAAI,CAAC,SAAS,CAAC,MAAM,EAAE;gBAC1C,cAAc,EAAE,MAAM,CAAC,SAAS,IAAI,IAAI,CAAC,MAAM,CAAC,SAAS;gBACzD,WAAW,EAAE,MAAM,CAAC,WAAW,IAAI,IAAI,CAAC,MAAM,CAAC,WAAW;gBAC1D,SAAS,EAAE,IAAI;gBACf,KAAK,EAAE,GAAG;aACX,CAAC,CAAC;YAEH,MAAM,aAAa,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,cAAc,CAAC;YAE/C,0CAA0C;YAC1C,MAAM,iBAAiB,GAAG,aAAa;iBACpC,KAAK,CAAC,eAAe,CAAC;iBACtB,GAAG,EAAE;gBACN,EAAE,KAAK,CAAC,SAAS,CAAC,CAAC,CAAC,CAAC;gBACrB,EAAE,IAAI,EAAE,IAAI,EAAE,CAAC;YAEjB,MAAM,OAAO,GAAG,IAAI,CAAC,GAAG,EAAE,GAAG,SAAS,CAAC;YAEvC,8CAA8C;YAC9C,MAAM,WAAW,GAAG,IAAI,CAAC,IAAI,CAAC,MAAM,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;YACjD,MAAM,YAAY,GAAG,IAAI,CAAC,IAAI,CAAC,iBAAiB,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;YAE7D,MAAM,OAAO,GAAmB,CAAC;oBAC/B,IAAI,EAAE,MAAM;oBACZ,IAAI,EAAE,iBAAiB;iBACxB,CAAC,CAAC;YAEH,OAAO;gBACL,EAAE,EAAE,QAAQ,IAAI,CAAC,GAAG,EAAE,EAAE;gBACxB,KAAK,EAAE,IAAI,CAAC,MAAM,CAAC,OAAO,IAAI,YAAY;gBAC1C,OAAO;gBACP,UAAU,EAAE,UAAU;gBACtB,KAAK,EAAE;oBACL,WAAW;oBACX,YAAY;iBACb;gBACD,QAAQ,EAAE;oBACR,QAAQ,EAAE,MAAM;oBAChB,KAAK,EAAE,IAAI,CAAC,MAAM,CAAC,OAAO;oBAC1B,OAAO;oBACP,IAAI,EAAE,CAAC,EAAE,0BAA0B;oBACnC,kBAAkB,EAAE,IAAI,CAAC,kBAAkB;iBAC5C;aACF,CAAC;QAEJ,CAAC;QAAC,OAAO,KAAK,EAAE,CAAC;YACf,MAAM,aAAa,GAAkB;gBACnC,IAAI,EAAE,oBAAoB;gBAC1B,OAAO,EAAE,0BAA0B,KAAK,EAAE;gBAC1C,QAAQ,EAAE,MAAM;gBAChB,SAAS,EAAE,IAAI;aAChB,CAAC;YACF,MAAM,aAAa,CAAC;QACtB,CAAC;IACH,CAAC;IAED;;OAEG;IACH,KAAK,CAAC,CAAC,MAAM,CAAC,MAAkB;QAC9B,MAAM,IAAI,CAAC,iBAAiB,EAAE,CAAC;QAE/B,MAAM,MAAM,GAAG,IAAI,CAAC,cAAc,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC;QAEpD,IAAI,CAAC;YACH,2DAA2D;YAC3D,4DAA4D;YAC5D,MAAM,MAAM,GAAG,MAAM,IAAI,CAAC,SAAS,CAAC,MAAM,EAAE;gBAC1C,cAAc,EAAE,MAAM,CAAC,SAAS,IAAI,IAAI,CAAC,MAAM,CAAC,SAAS;gBACzD,WAAW,EAAE,MAAM,CAAC,WAAW,IAAI,IAAI,CAAC,MAAM,CAAC,WAAW;gBAC1D,SAAS,EAAE,IAAI;gBACf,KAAK,EAAE,GAAG;aACX,CAAC,CAAC;YAEH,MAAM,aAAa,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,cAAc,CAAC;YAC/C,MAAM,iBAAiB,GAAG,aAAa;iBACpC,KAAK,CAAC,eAAe,CAAC;iBACtB,GAAG,EAAE;gBACN,EAAE,KAAK,CAAC,SAAS,CAAC,CAAC,CAAC,CAAC;gBACrB,EAAE,IAAI,EAAE,IAAI,EAAE,CAAC;YAEjB,8CAA8C;YAC9C,MAAM,KAAK,GAAG,iBAAiB,CAAC,KAAK,CAAC,GAAG,CAAC,CAAC;YAC3C,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;gBACtC,MAAM,KAAK,GAAG,KAAK,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,GAAG,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC;gBAC3D,MAAM;oBACJ,IAAI,EAAE,qBAAqB;oBAC3B,KAAK,EAAE;wBACL,IAAI,EAAE,YAAY;wBAClB,IAAI,EAAE,KAAK;qBACZ;iBACF,CAAC;gBAEF,yCAAyC;gBACzC,MAAM,IAAI,OAAO,CAAC,OAAO,CAAC,EAAE,CAAC,UAAU,CAAC,OAAO,EAAE,EAAE,CAAC,CAAC,CAAC;YACxD,CAAC;YAED,MAAM;gBACJ,IAAI,EAAE,cAAc;aACrB,CAAC;QAEJ,CAAC;QAAC,OAAO,KAAK,EAAE,CAAC;YACf,MAAM,aAAa,GAAkB;gBACnC,IAAI,EAAE,iBAAiB;gBACvB,OAAO,EAAE,0BAA0B,KAAK,EAAE;gBAC1C,QAAQ,EAAE,MAAM;gBAChB,SAAS,EAAE,IAAI;aAChB,CAAC;YACF,MAAM,aAAa,CAAC;QACtB,CAAC;IACH,CAAC;IAED;;OAEG;IACH,oBAAoB,CAAC,QAAkB;QACrC,MAAM,SAAS,GAAG,CAAC,MAAM,EAAE,QAAQ,CAAC,CAAC;QACrC,OAAO,QAAQ,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,SAAS,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC;IACpD,CAAC;IAED;;OAEG;IACH,YAAY;QACV,OAAO;YACL,OAAO,EAAE,IAAI,CAAC,MAAM,CAAC,OAAO;YAC5B,kBAAkB,EAAE,IAAI,CAAC,kBAAkB;YAC3C,WAAW,EAAE,IAAI,CAAC,kBAAkB,CAAC,QAAQ,CAAC,MAAM,CAAC,IAAI,IAAI,CAAC,kBAAkB,CAAC,QAAQ,CAAC,KAAK,CAAC;YAChG,WAAW,EAAE,IAAI,CAAC,SAAS,KAAK,IAAI;SACrC,CAAC;IACJ,CAAC;IAED;;OAEG;IACH,KAAK,CAAC,OAAO;QACX,IAAI,IAAI,CAAC,SAAS,EAAE,CAAC;YACnB,IAAI,CAAC,SAAS,GAAG,IAAI,CAAC;QACxB,CAAC;QACD,IAAI,IAAI,CAAC,OAAO,EAAE,CAAC;YACjB,MAAM,IAAI,CAAC,OAAO,CAAC,OAAO,EAAE,CAAC;YAC7B,IAAI,CAAC,OAAO,GAAG,IAAI,CAAC;QACtB,CAAC;IACH,CAAC;CACF","sourcesContent":["/**\n * ONNX Runtime Provider for Local Model Inference\n *\n * Supports CPU and GPU execution providers for optimized local inference\n * Compatible with Phi-3, Llama, and other ONNX models\n */\n\nimport type {\n  LLMProvider,\n  ChatParams,\n  ChatResponse,\n  StreamChunk,\n  ProviderError,\n  Message,\n  ContentBlock\n} from '../types.js';\n\n// Dynamic imports for optional ONNX dependencies\nlet ort: any;\nlet transformers: any;\n\nasync function ensureOnnxDependencies() {\n  if (!ort) {\n    try {\n      const ortModule = await import('onnxruntime-node' as any);\n      ort = ortModule;\n    } catch (e) {\n      throw new Error('onnxruntime-node not installed. Run: npm install onnxruntime-node');\n    }\n  }\n  if (!transformers) {\n    try {\n      const transformersModule = await import('@xenova/transformers' as any);\n      transformers = transformersModule;\n      transformers.env.allowLocalModels = true;\n    } catch (e) {\n      throw new Error('@xenova/transformers not installed. Run: npm install @xenova/transformers');\n    }\n  }\n}\n\nexport interface ONNXConfig {\n  modelPath?: string;\n  modelId?: string; // HuggingFace model ID\n  executionProviders?: string[];\n  sessionOptions?: any;\n  maxTokens?: number;\n  temperature?: number;\n}\n\nexport class ONNXProvider implements LLMProvider {\n  name = 'onnx';\n  type = 'custom' as const;\n  supportsStreaming = true;\n  supportsTools = false;\n  supportsMCP = false;\n\n  private session: any = null;\n  private generator: any = null;\n  private config: ONNXConfig;\n  private executionProviders: string[] = [];\n\n  constructor(config: ONNXConfig = {}) {\n    this.config = {\n      modelId: config.modelId || 'Xenova/Phi-3-mini-4k-instruct',\n      maxTokens: config.maxTokens || 512,\n      temperature: config.temperature || 0.7,\n      ...config\n    };\n  }\n\n  /**\n   * Detect available execution providers\n   */\n  private async detectExecutionProviders(): Promise<string[]> {\n    const providers: string[] = [];\n\n    // Try CUDA for NVIDIA GPUs\n    try {\n      if (process.platform === 'linux') {\n        providers.push('cuda');\n        this.executionProviders.push('cuda');\n      }\n    } catch (e) {\n      // CUDA not available\n    }\n\n    // Try DirectML for Windows GPUs\n    try {\n      if (process.platform === 'win32') {\n        providers.push('dml');\n        this.executionProviders.push('dml');\n      }\n    } catch (e) {\n      // DirectML not available\n    }\n\n    // Always fallback to CPU\n    providers.push('cpu');\n    this.executionProviders.push('cpu');\n\n    console.log(`ðŸ”§ ONNX Execution Providers: ${this.executionProviders.join(', ')}`);\n\n    return providers;\n  }\n\n  /**\n   * Initialize ONNX session with model\n   */\n  private async initializeSession(): Promise<void> {\n    if (this.generator) return;\n\n    try {\n      await ensureOnnxDependencies();\n\n      console.log(`ðŸ“¦ Loading ONNX model: ${this.config.modelId}`);\n\n      // Use Transformers.js for easier model loading\n      this.generator = await transformers.pipeline(\n        'text-generation',\n        this.config.modelId,\n        {\n          quantized: true, // Use quantized models for better CPU performance\n        }\n      );\n\n      console.log(`âœ… ONNX model loaded successfully`);\n\n    } catch (error) {\n      const providerError: ProviderError = {\n        name: 'ONNXInitError',\n        message: `Failed to initialize ONNX model: ${error}`,\n        provider: 'onnx',\n        retryable: false\n      };\n      throw providerError;\n    }\n  }\n\n  /**\n   * Format messages for model input\n   */\n  private formatMessages(messages: Message[]): string {\n    // Simple chat template for Phi-3\n    let prompt = '';\n\n    for (const msg of messages) {\n      const content = typeof msg.content === 'string'\n        ? msg.content\n        : msg.content.map(c => c.type === 'text' ? c.text : '').join('');\n\n      if (msg.role === 'user') {\n        prompt += `<|user|>\\n${content}<|end|>\\n`;\n      } else if (msg.role === 'assistant') {\n        prompt += `<|assistant|>\\n${content}<|end|>\\n`;\n      } else if (msg.role === 'system') {\n        prompt += `<|system|>\\n${content}<|end|>\\n`;\n      }\n    }\n\n    prompt += '<|assistant|>\\n';\n    return prompt;\n  }\n\n  /**\n   * Chat completion\n   */\n  async chat(params: ChatParams): Promise<ChatResponse> {\n    await this.initializeSession();\n\n    const startTime = Date.now();\n    const prompt = this.formatMessages(params.messages);\n\n    try {\n      const result = await this.generator(prompt, {\n        max_new_tokens: params.maxTokens || this.config.maxTokens,\n        temperature: params.temperature || this.config.temperature,\n        do_sample: true,\n        top_p: 0.9,\n      });\n\n      const generatedText = result[0].generated_text;\n\n      // Extract only the new assistant response\n      const assistantResponse = generatedText\n        .split('<|assistant|>')\n        .pop()\n        ?.split('<|end|>')[0]\n        ?.trim() || '';\n\n      const latency = Date.now() - startTime;\n\n      // Estimate token counts (rough approximation)\n      const inputTokens = Math.ceil(prompt.length / 4);\n      const outputTokens = Math.ceil(assistantResponse.length / 4);\n\n      const content: ContentBlock[] = [{\n        type: 'text',\n        text: assistantResponse\n      }];\n\n      return {\n        id: `onnx-${Date.now()}`,\n        model: this.config.modelId || 'onnx-model',\n        content,\n        stopReason: 'end_turn',\n        usage: {\n          inputTokens,\n          outputTokens\n        },\n        metadata: {\n          provider: 'onnx',\n          model: this.config.modelId,\n          latency,\n          cost: 0, // Local inference is free\n          executionProviders: this.executionProviders\n        }\n      };\n\n    } catch (error) {\n      const providerError: ProviderError = {\n        name: 'ONNXInferenceError',\n        message: `ONNX inference failed: ${error}`,\n        provider: 'onnx',\n        retryable: true\n      };\n      throw providerError;\n    }\n  }\n\n  /**\n   * Streaming generation\n   */\n  async *stream(params: ChatParams): AsyncGenerator<StreamChunk> {\n    await this.initializeSession();\n\n    const prompt = this.formatMessages(params.messages);\n\n    try {\n      // Note: Transformers.js doesn't natively support streaming\n      // We'll simulate it by yielding tokens as they're generated\n      const result = await this.generator(prompt, {\n        max_new_tokens: params.maxTokens || this.config.maxTokens,\n        temperature: params.temperature || this.config.temperature,\n        do_sample: true,\n        top_p: 0.9,\n      });\n\n      const generatedText = result[0].generated_text;\n      const assistantResponse = generatedText\n        .split('<|assistant|>')\n        .pop()\n        ?.split('<|end|>')[0]\n        ?.trim() || '';\n\n      // Simulate streaming by chunking the response\n      const words = assistantResponse.split(' ');\n      for (let i = 0; i < words.length; i++) {\n        const chunk = words[i] + (i < words.length - 1 ? ' ' : '');\n        yield {\n          type: 'content_block_delta',\n          delta: {\n            type: 'text_delta',\n            text: chunk\n          }\n        };\n\n        // Small delay to simulate real streaming\n        await new Promise(resolve => setTimeout(resolve, 10));\n      }\n\n      yield {\n        type: 'message_stop'\n      };\n\n    } catch (error) {\n      const providerError: ProviderError = {\n        name: 'ONNXStreamError',\n        message: `ONNX streaming failed: ${error}`,\n        provider: 'onnx',\n        retryable: true\n      };\n      throw providerError;\n    }\n  }\n\n  /**\n   * Validate capabilities\n   */\n  validateCapabilities(features: string[]): boolean {\n    const supported = ['chat', 'stream'];\n    return features.every(f => supported.includes(f));\n  }\n\n  /**\n   * Get model info\n   */\n  getModelInfo() {\n    return {\n      modelId: this.config.modelId,\n      executionProviders: this.executionProviders,\n      supportsGPU: this.executionProviders.includes('cuda') || this.executionProviders.includes('dml'),\n      initialized: this.generator !== null\n    };\n  }\n\n  /**\n   * Cleanup resources\n   */\n  async dispose(): Promise<void> {\n    if (this.generator) {\n      this.generator = null;\n    }\n    if (this.session) {\n      await this.session.release();\n      this.session = null;\n    }\n  }\n}\n"]}
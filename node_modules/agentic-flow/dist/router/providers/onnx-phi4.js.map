{"version":3,"file":"onnx-phi4.js","sourceRoot":"","sources":["../../../src/router/providers/onnx-phi4.ts"],"names":[],"mappings":"AAAA;;;;;GAKG;AAEH,OAAO,EAAE,WAAW,EAAE,MAAM,wBAAwB,CAAC;AAmBrD,MAAM,OAAO,gBAAgB;IAC3B,IAAI,GAAG,WAAW,CAAC;IACnB,IAAI,GAAG,QAAiB,CAAC;IACzB,iBAAiB,GAAG,IAAI,CAAC;IACzB,aAAa,GAAG,KAAK,CAAC;IACtB,WAAW,GAAG,KAAK,CAAC;IAEZ,MAAM,CAA2B;IACjC,EAAE,CAAc;IAChB,SAAS,GAAG,4EAA4E,CAAC;IAEjG,YAAY,SAAyB,EAAE;QACrC,IAAI,CAAC,MAAM,GAAG;YACZ,OAAO,EAAE,MAAM,CAAC,OAAO,IAAI,kCAAkC;YAC7D,YAAY,EAAE,MAAM,CAAC,YAAY,IAAI,KAAK,EAAE,8CAA8C;YAC1F,iBAAiB,EAAE,MAAM,CAAC,iBAAiB,IAAI,OAAO,CAAC,GAAG,CAAC,mBAAmB,IAAI,EAAE;YACpF,SAAS,EAAE,MAAM,CAAC,SAAS,IAAI,GAAG;YAClC,WAAW,EAAE,MAAM,CAAC,WAAW,IAAI,GAAG;SACvC,CAAC;QAEF,IAAI,CAAC,EAAE,GAAG,IAAI,WAAW,CAAC,IAAI,CAAC,MAAM,CAAC,iBAAiB,CAAC,CAAC;IAC3D,CAAC;IAED;;OAEG;IACK,cAAc,CAAC,QAAmB;QACxC,IAAI,MAAM,GAAG,EAAE,CAAC;QAEhB,KAAK,MAAM,GAAG,IAAI,QAAQ,EAAE,CAAC;YAC3B,MAAM,OAAO,GAAG,OAAO,GAAG,CAAC,OAAO,KAAK,QAAQ;gBAC7C,CAAC,CAAC,GAAG,CAAC,OAAO;gBACb,CAAC,CAAC,GAAG,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,IAAI,KAAK,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;YAEnE,IAAI,GAAG,CAAC,IAAI,KAAK,QAAQ,EAAE,CAAC;gBAC1B,MAAM,IAAI,eAAe,OAAO,WAAW,CAAC;YAC9C,CAAC;iBAAM,IAAI,GAAG,CAAC,IAAI,KAAK,MAAM,EAAE,CAAC;gBAC/B,MAAM,IAAI,aAAa,OAAO,WAAW,CAAC;YAC5C,CAAC;iBAAM,IAAI,GAAG,CAAC,IAAI,KAAK,WAAW,EAAE,CAAC;gBACpC,MAAM,IAAI,kBAAkB,OAAO,WAAW,CAAC;YACjD,CAAC;QACH,CAAC;QAED,MAAM,IAAI,iBAAiB,CAAC;QAC5B,OAAO,MAAM,CAAC;IAChB,CAAC;IAED;;OAEG;IACK,KAAK,CAAC,UAAU,CAAC,MAAkB;QACzC,MAAM,SAAS,GAAG,IAAI,CAAC,GAAG,EAAE,CAAC;QAC7B,MAAM,MAAM,GAAG,IAAI,CAAC,cAAc,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC;QAEpD,IAAI,CAAC;YACH,MAAM,MAAM,GAAG,MAAM,IAAI,CAAC,EAAE,CAAC,cAAc,CAAC;gBAC1C,KAAK,EAAE,IAAI,CAAC,MAAM,CAAC,OAAO;gBAC1B,MAAM,EAAE,MAAM;gBACd,UAAU,EAAE;oBACV,cAAc,EAAE,MAAM,CAAC,SAAS,IAAI,IAAI,CAAC,MAAM,CAAC,SAAS;oBACzD,WAAW,EAAE,MAAM,CAAC,WAAW,IAAI,IAAI,CAAC,MAAM,CAAC,WAAW;oBAC1D,gBAAgB,EAAE,KAAK;oBACvB,SAAS,EAAE,IAAI;oBACf,KAAK,EAAE,GAAG;iBACX;aACF,CAAC,CAAC;YAEH,MAAM,OAAO,GAAG,IAAI,CAAC,GAAG,EAAE,GAAG,SAAS,CAAC;YACvC,MAAM,aAAa,GAAG,MAAM,CAAC,cAAc,CAAC;YAE5C,wBAAwB;YACxB,MAAM,iBAAiB,GAAG,aAAa;iBACpC,KAAK,CAAC,SAAS,CAAC,CAAC,CAAC,CAAC;iBACnB,IAAI,EAAE,CAAC;YAEV,wBAAwB;YACxB,MAAM,WAAW,GAAG,IAAI,CAAC,IAAI,CAAC,MAAM,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;YACjD,MAAM,YAAY,GAAG,IAAI,CAAC,IAAI,CAAC,iBAAiB,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;YAE7D,MAAM,OAAO,GAAmB,CAAC;oBAC/B,IAAI,EAAE,MAAM;oBACZ,IAAI,EAAE,iBAAiB;iBACxB,CAAC,CAAC;YAEH,OAAO;gBACL,EAAE,EAAE,aAAa,IAAI,CAAC,GAAG,EAAE,EAAE;gBAC7B,KAAK,EAAE,IAAI,CAAC,MAAM,CAAC,OAAO;gBAC1B,OAAO;gBACP,UAAU,EAAE,UAAU;gBACtB,KAAK,EAAE;oBACL,WAAW;oBACX,YAAY;iBACb;gBACD,QAAQ,EAAE;oBACR,QAAQ,EAAE,WAAW;oBACrB,KAAK,EAAE,IAAI,CAAC,MAAM,CAAC,OAAO;oBAC1B,OAAO;oBACP,IAAI,EAAE,YAAY,GAAG,QAAQ,EAAE,iBAAiB;oBAChD,IAAI,EAAE,KAAK;iBACZ;aACF,CAAC;QAEJ,CAAC;QAAC,OAAO,KAAK,EAAE,CAAC;YACf,MAAM,aAAa,GAAkB;gBACnC,IAAI,EAAE,kBAAkB;gBACxB,OAAO,EAAE,qCAAqC,KAAK,EAAE;gBACrD,QAAQ,EAAE,WAAW;gBACrB,SAAS,EAAE,IAAI;aAChB,CAAC;YACF,MAAM,aAAa,CAAC;QACtB,CAAC;IACH,CAAC;IAED;;OAEG;IACK,KAAK,CAAC,WAAW,CAAC,MAAkB;QAC1C,MAAM,IAAI,KAAK,CAAC,2EAA2E,CAAC,CAAC;IAC/F,CAAC;IAED;;OAEG;IACH,KAAK,CAAC,IAAI,CAAC,MAAkB;QAC3B,IAAI,IAAI,CAAC,MAAM,CAAC,YAAY,EAAE,CAAC;YAC7B,OAAO,IAAI,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;QAClC,CAAC;aAAM,CAAC;YACN,OAAO,IAAI,CAAC,UAAU,CAAC,MAAM,CAAC,CAAC;QACjC,CAAC;IACH,CAAC;IAED;;OAEG;IACH,KAAK,CAAC,CAAC,MAAM,CAAC,MAAkB;QAC9B,MAAM,MAAM,GAAG,IAAI,CAAC,cAAc,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC;QAEpD,IAAI,CAAC;YACH,MAAM,MAAM,GAAG,IAAI,CAAC,EAAE,CAAC,oBAAoB,CAAC;gBAC1C,KAAK,EAAE,IAAI,CAAC,MAAM,CAAC,OAAO;gBAC1B,MAAM,EAAE,MAAM;gBACd,UAAU,EAAE;oBACV,cAAc,EAAE,MAAM,CAAC,SAAS,IAAI,IAAI,CAAC,MAAM,CAAC,SAAS;oBACzD,WAAW,EAAE,MAAM,CAAC,WAAW,IAAI,IAAI,CAAC,MAAM,CAAC,WAAW;oBAC1D,gBAAgB,EAAE,KAAK;iBACxB;aACF,CAAC,CAAC;YAEH,IAAI,KAAK,EAAE,MAAM,KAAK,IAAI,MAAM,EAAE,CAAC;gBACjC,IAAI,KAAK,CAAC,KAAK,CAAC,IAAI,EAAE,CAAC;oBACrB,MAAM;wBACJ,IAAI,EAAE,qBAAqB;wBAC3B,KAAK,EAAE;4BACL,IAAI,EAAE,YAAY;4BAClB,IAAI,EAAE,KAAK,CAAC,KAAK,CAAC,IAAI;yBACvB;qBACF,CAAC;gBACJ,CAAC;YACH,CAAC;YAED,MAAM;gBACJ,IAAI,EAAE,cAAc;aACrB,CAAC;QAEJ,CAAC;QAAC,OAAO,KAAK,EAAE,CAAC;YACf,MAAM,aAAa,GAAkB;gBACnC,IAAI,EAAE,qBAAqB;gBAC3B,OAAO,EAAE,qBAAqB,KAAK,EAAE;gBACrC,QAAQ,EAAE,WAAW;gBACrB,SAAS,EAAE,IAAI;aAChB,CAAC;YACF,MAAM,aAAa,CAAC;QACtB,CAAC;IACH,CAAC;IAED;;OAEG;IACH,oBAAoB,CAAC,QAAkB;QACrC,MAAM,SAAS,GAAG,CAAC,MAAM,EAAE,QAAQ,CAAC,CAAC;QACrC,OAAO,QAAQ,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,SAAS,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC;IACpD,CAAC;IAED;;OAEG;IACH,YAAY;QACV,OAAO;YACL,OAAO,EAAE,IAAI,CAAC,MAAM,CAAC,OAAO;YAC5B,IAAI,EAAE,IAAI,CAAC,MAAM,CAAC,YAAY,CAAC,CAAC,CAAC,YAAY,CAAC,CAAC,CAAC,KAAK;YACrD,sBAAsB,EAAE,KAAK,EAAE,+CAA+C;YAC9E,SAAS,EAAE,IAAI,CAAC,SAAS;YACzB,MAAM,EAAE,IAAI,CAAC,MAAM,CAAC,iBAAiB,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,SAAS;SAC1D,CAAC;IACJ,CAAC;IAED;;OAEG;IACH,OAAO,CAAC,YAAqB;QAC3B,IAAI,CAAC,MAAM,CAAC,YAAY,GAAG,YAAY,CAAC;IAC1C,CAAC;CACF","sourcesContent":["/**\n * ONNX Runtime Provider for Phi-4 Model\n *\n * Hybrid implementation with fallback to HuggingFace Inference API\n * when local ONNX model is not available\n */\n\nimport { HfInference } from '@huggingface/inference';\nimport type {\n  LLMProvider,\n  ChatParams,\n  ChatResponse,\n  StreamChunk,\n  ProviderError,\n  Message,\n  ContentBlock\n} from '../types.js';\n\nexport interface ONNXPhi4Config {\n  modelId?: string;\n  useLocalONNX?: boolean;\n  huggingfaceApiKey?: string;\n  maxTokens?: number;\n  temperature?: number;\n}\n\nexport class ONNXPhi4Provider implements LLMProvider {\n  name = 'onnx-phi4';\n  type = 'custom' as const;\n  supportsStreaming = true;\n  supportsTools = false;\n  supportsMCP = false;\n\n  private config: Required<ONNXPhi4Config>;\n  private hf: HfInference;\n  private modelPath = './models/phi-4/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx';\n\n  constructor(config: ONNXPhi4Config = {}) {\n    this.config = {\n      modelId: config.modelId || 'microsoft/Phi-3-mini-4k-instruct',\n      useLocalONNX: config.useLocalONNX ?? false, // Default to API until local model downloaded\n      huggingfaceApiKey: config.huggingfaceApiKey || process.env.HUGGINGFACE_API_KEY || '',\n      maxTokens: config.maxTokens || 512,\n      temperature: config.temperature || 0.7\n    };\n\n    this.hf = new HfInference(this.config.huggingfaceApiKey);\n  }\n\n  /**\n   * Format messages for Phi-4 chat template\n   */\n  private formatMessages(messages: Message[]): string {\n    let prompt = '';\n\n    for (const msg of messages) {\n      const content = typeof msg.content === 'string'\n        ? msg.content\n        : msg.content.map(c => c.type === 'text' ? c.text : '').join('');\n\n      if (msg.role === 'system') {\n        prompt += `<|system|>\\n${content}<|end|>\\n`;\n      } else if (msg.role === 'user') {\n        prompt += `<|user|>\\n${content}<|end|>\\n`;\n      } else if (msg.role === 'assistant') {\n        prompt += `<|assistant|>\\n${content}<|end|>\\n`;\n      }\n    }\n\n    prompt += '<|assistant|>\\n';\n    return prompt;\n  }\n\n  /**\n   * Chat completion via HuggingFace Inference API\n   */\n  private async chatViaAPI(params: ChatParams): Promise<ChatResponse> {\n    const startTime = Date.now();\n    const prompt = this.formatMessages(params.messages);\n\n    try {\n      const result = await this.hf.textGeneration({\n        model: this.config.modelId,\n        inputs: prompt,\n        parameters: {\n          max_new_tokens: params.maxTokens || this.config.maxTokens,\n          temperature: params.temperature || this.config.temperature,\n          return_full_text: false,\n          do_sample: true,\n          top_p: 0.9\n        }\n      });\n\n      const latency = Date.now() - startTime;\n      const generatedText = result.generated_text;\n\n      // Clean up the response\n      const assistantResponse = generatedText\n        .split('<|end|>')[0]\n        .trim();\n\n      // Estimate token counts\n      const inputTokens = Math.ceil(prompt.length / 4);\n      const outputTokens = Math.ceil(assistantResponse.length / 4);\n\n      const content: ContentBlock[] = [{\n        type: 'text',\n        text: assistantResponse\n      }];\n\n      return {\n        id: `onnx-phi4-${Date.now()}`,\n        model: this.config.modelId,\n        content,\n        stopReason: 'end_turn',\n        usage: {\n          inputTokens,\n          outputTokens\n        },\n        metadata: {\n          provider: 'onnx-phi4',\n          model: this.config.modelId,\n          latency,\n          cost: outputTokens * 0.000002, // Rough estimate\n          mode: 'api'\n        }\n      };\n\n    } catch (error) {\n      const providerError: ProviderError = {\n        name: 'ONNXPhi4APIError',\n        message: `HuggingFace API inference failed: ${error}`,\n        provider: 'onnx-phi4',\n        retryable: true\n      };\n      throw providerError;\n    }\n  }\n\n  /**\n   * Chat completion via local ONNX (not yet implemented)\n   */\n  private async chatViaONNX(params: ChatParams): Promise<ChatResponse> {\n    throw new Error('Local ONNX inference not yet implemented. Download model.onnx.data first.');\n  }\n\n  /**\n   * Chat completion (uses API or local ONNX based on config)\n   */\n  async chat(params: ChatParams): Promise<ChatResponse> {\n    if (this.config.useLocalONNX) {\n      return this.chatViaONNX(params);\n    } else {\n      return this.chatViaAPI(params);\n    }\n  }\n\n  /**\n   * Streaming generation via HuggingFace API\n   */\n  async *stream(params: ChatParams): AsyncGenerator<StreamChunk> {\n    const prompt = this.formatMessages(params.messages);\n\n    try {\n      const stream = this.hf.textGenerationStream({\n        model: this.config.modelId,\n        inputs: prompt,\n        parameters: {\n          max_new_tokens: params.maxTokens || this.config.maxTokens,\n          temperature: params.temperature || this.config.temperature,\n          return_full_text: false\n        }\n      });\n\n      for await (const chunk of stream) {\n        if (chunk.token.text) {\n          yield {\n            type: 'content_block_delta',\n            delta: {\n              type: 'text_delta',\n              text: chunk.token.text\n            }\n          };\n        }\n      }\n\n      yield {\n        type: 'message_stop'\n      };\n\n    } catch (error) {\n      const providerError: ProviderError = {\n        name: 'ONNXPhi4StreamError',\n        message: `Streaming failed: ${error}`,\n        provider: 'onnx-phi4',\n        retryable: true\n      };\n      throw providerError;\n    }\n  }\n\n  /**\n   * Validate capabilities\n   */\n  validateCapabilities(features: string[]): boolean {\n    const supported = ['chat', 'stream'];\n    return features.every(f => supported.includes(f));\n  }\n\n  /**\n   * Get model info\n   */\n  getModelInfo() {\n    return {\n      modelId: this.config.modelId,\n      mode: this.config.useLocalONNX ? 'local-onnx' : 'api',\n      supportsLocalInference: false, // Will be true when model.onnx.data downloaded\n      modelPath: this.modelPath,\n      apiKey: this.config.huggingfaceApiKey ? '***' : undefined\n    };\n  }\n\n  /**\n   * Switch between API and local ONNX\n   */\n  setMode(useLocalONNX: boolean) {\n    this.config.useLocalONNX = useLocalONNX;\n  }\n}\n"]}
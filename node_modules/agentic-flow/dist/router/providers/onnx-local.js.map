{"version":3,"file":"onnx-local.js","sourceRoot":"","sources":["../../../src/router/providers/onnx-local.ts"],"names":[],"mappings":"AAAA;;;;GAIG;AAEH,OAAO,KAAK,GAAG,MAAM,kBAAkB,CAAC;AAGxC,OAAO,EAAE,YAAY,EAAE,MAAM,UAAU,CAAC;AACxC,OAAO,EAAE,eAAe,EAAE,eAAe,EAAE,MAAM,iCAAiC,CAAC;AAkBnF,MAAM,OAAO,iBAAiB;IAC5B,IAAI,GAAG,YAAY,CAAC;IACpB,IAAI,GAAG,QAAiB,CAAC;IACzB,iBAAiB,GAAG,KAAK,CAAC,CAAC,mDAAmD;IAC9E,aAAa,GAAG,KAAK,CAAC;IACtB,WAAW,GAAG,KAAK,CAAC;IAEZ,OAAO,GAAgC,IAAI,CAAC;IAC5C,MAAM,CAA4B;IAClC,SAAS,GAAQ,IAAI,CAAC;IACtB,QAAQ,GAAQ,IAAI,CAAC;IAE7B,YAAY,SAA0B,EAAE;QACtC,IAAI,CAAC,MAAM,GAAG;YACZ,SAAS,EAAE,MAAM,CAAC,SAAS,IAAI,4EAA4E;YAC3G,kBAAkB,EAAE,MAAM,CAAC,kBAAkB,IAAI,CAAC,KAAK,CAAC;YACxD,SAAS,EAAE,MAAM,CAAC,SAAS,IAAI,GAAG;YAClC,WAAW,EAAE,MAAM,CAAC,WAAW,IAAI,GAAG;SACvC,CAAC;IACJ,CAAC;IAED;;OAEG;IACK,KAAK,CAAC,aAAa;QACzB,IAAI,IAAI,CAAC,QAAQ;YAAE,OAAO;QAE1B,IAAI,CAAC;YACH,qDAAqD;YACrD,IAAI,CAAC,QAAQ,GAAG,YAAY,CAAC,aAAa,CAAC,CAAC;YAE5C,OAAO,CAAC,GAAG,CAAC,2CAA2C,CAAC,CAAC;QAC3D,CAAC;QAAC,OAAO,KAAK,EAAE,CAAC;YACf,OAAO,CAAC,KAAK,CAAC,4BAA4B,EAAE,KAAK,CAAC,CAAC;YACnD,MAAM,IAAI,KAAK,CAAC,6BAA6B,KAAK,EAAE,CAAC,CAAC;QACxD,CAAC;IACH,CAAC;IAED;;OAEG;IACK,MAAM,CAAC,IAAY;QACzB,OAAO,KAAK,CAAC,IAAI,CAAC,IAAI,CAAC,QAAQ,CAAC,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC;IAChD,CAAC;IAED;;OAEG;IACK,MAAM,CAAC,GAAa;QAC1B,IAAI,CAAC;YACH,MAAM,OAAO,GAAG,IAAI,CAAC,QAAQ,CAAC,MAAM,CAAC,IAAI,WAAW,CAAC,GAAG,CAAC,CAAC,CAAC;YAC3D,6CAA6C;YAC7C,IAAI,OAAO,OAAO,KAAK,QAAQ,EAAE,CAAC;gBAChC,OAAO,OAAO,CAAC;YACjB,CAAC;iBAAM,IAAI,OAAO,YAAY,UAAU,IAAI,OAAO,YAAY,MAAM,EAAE,CAAC;gBACtE,OAAO,IAAI,WAAW,EAAE,CAAC,MAAM,CAAC,OAAO,CAAC,CAAC;YAC3C,CAAC;YACD,OAAO,MAAM,CAAC,OAAO,CAAC,CAAC;QACzB,CAAC;QAAC,OAAO,KAAK,EAAE,CAAC;YACf,OAAO,CAAC,IAAI,CAAC,kCAAkC,EAAE,KAAK,CAAC,CAAC;YACxD,OAAO,GAAG,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;QACvB,CAAC;IACH,CAAC;IAED;;OAEG;IACK,KAAK,CAAC,iBAAiB;QAC7B,IAAI,IAAI,CAAC,OAAO;YAAE,OAAO;QAEzB,IAAI,CAAC;YACH,6BAA6B;YAC7B,OAAO,CAAC,GAAG,CAAC,qCAAqC,CAAC,CAAC;YAEnD,MAAM,SAAS,GAAG,MAAM,eAAe,CAAC,CAAC,QAAQ,EAAE,EAAE;gBACnD,IAAI,QAAQ,CAAC,UAAU,GAAG,EAAE,GAAG,CAAC,EAAE,CAAC,CAAC,iBAAiB;oBACnD,OAAO,CAAC,GAAG,CAAC,sBAAsB,eAAe,CAAC,cAAc,CAAC,QAAQ,CAAC,EAAE,CAAC,CAAC;gBAChF,CAAC;YACH,CAAC,CAAC,CAAC;YAEH,uCAAuC;YACvC,IAAI,CAAC,MAAM,CAAC,SAAS,GAAG,SAAS,CAAC;YAElC,OAAO,CAAC,GAAG,CAAC,0BAA0B,IAAI,CAAC,MAAM,CAAC,SAAS,EAAE,CAAC,CAAC;YAE/D,IAAI,CAAC,OAAO,GAAG,MAAM,GAAG,CAAC,gBAAgB,CAAC,MAAM,CAC9C,IAAI,CAAC,MAAM,CAAC,SAAS,EACrB;gBACE,kBAAkB,EAAE,IAAI,CAAC,MAAM,CAAC,kBAAyB;gBACzD,sBAAsB,EAAE,KAAK;gBAC7B,iBAAiB,EAAE,IAAI;gBACvB,gBAAgB,EAAE,IAAI;aACvB,CACF,CAAC;YAEF,OAAO,CAAC,GAAG,CAAC,qBAAqB,CAAC,CAAC;YACnC,OAAO,CAAC,GAAG,CAAC,2BAA2B,IAAI,CAAC,MAAM,CAAC,kBAAkB,CAAC,IAAI,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;YAEpF,iBAAiB;YACjB,MAAM,IAAI,CAAC,aAAa,EAAE,CAAC;QAE7B,CAAC;QAAC,OAAO,KAAK,EAAE,CAAC;YACf,MAAM,aAAa,GAAkB;gBACnC,IAAI,EAAE,eAAe;gBACrB,OAAO,EAAE,oCAAoC,KAAK,EAAE;gBACpD,QAAQ,EAAE,YAAY;gBACtB,SAAS,EAAE,KAAK;aACjB,CAAC;YACF,MAAM,aAAa,CAAC;QACtB,CAAC;IACH,CAAC;IAED;;OAEG;IACK,cAAc,CAAC,QAAmB;QACxC,IAAI,MAAM,GAAG,EAAE,CAAC;QAEhB,KAAK,MAAM,GAAG,IAAI,QAAQ,EAAE,CAAC;YAC3B,MAAM,OAAO,GAAG,OAAO,GAAG,CAAC,OAAO,KAAK,QAAQ;gBAC7C,CAAC,CAAC,GAAG,CAAC,OAAO;gBACb,CAAC,CAAC,GAAG,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,IAAI,KAAK,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;YAEnE,IAAI,GAAG,CAAC,IAAI,KAAK,QAAQ,EAAE,CAAC;gBAC1B,MAAM,IAAI,eAAe,OAAO,WAAW,CAAC;YAC9C,CAAC;iBAAM,IAAI,GAAG,CAAC,IAAI,KAAK,MAAM,EAAE,CAAC;gBAC/B,MAAM,IAAI,aAAa,OAAO,WAAW,CAAC;YAC5C,CAAC;iBAAM,IAAI,GAAG,CAAC,IAAI,KAAK,WAAW,EAAE,CAAC;gBACpC,MAAM,IAAI,kBAAkB,OAAO,WAAW,CAAC;YACjD,CAAC;QACH,CAAC;QAED,MAAM,IAAI,iBAAiB,CAAC;QAC5B,OAAO,MAAM,CAAC;IAChB,CAAC;IAED;;;OAGG;IACK,iBAAiB,CAAC,SAAiB,EAAE,cAAsB;QACjE,MAAM,SAAS,GAAG,EAAE,CAAC;QACrB,MAAM,UAAU,GAAG,CAAC,CAAC;QACrB,MAAM,OAAO,GAAG,GAAG,CAAC,CAAC,kBAAkB;QACvC,MAAM,OAAO,GAA+B,EAAE,CAAC;QAE/C,wDAAwD;QACxD,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,SAAS,EAAE,CAAC,EAAE,EAAE,CAAC;YACnC,uDAAuD;YACvD,MAAM,UAAU,GAAG,IAAI,YAAY,CAAC,CAAC,CAAC,CAAC;YAEvC,OAAO,CAAC,mBAAmB,CAAC,MAAM,CAAC,GAAG,IAAI,GAAG,CAAC,MAAM,CAClD,SAAS,EACT,UAAU,EACV,CAAC,SAAS,EAAE,UAAU,EAAE,CAAC,EAAE,OAAO,CAAC,CACpC,CAAC;YAEF,OAAO,CAAC,mBAAmB,CAAC,QAAQ,CAAC,GAAG,IAAI,GAAG,CAAC,MAAM,CACpD,SAAS,EACT,UAAU,EACV,CAAC,SAAS,EAAE,UAAU,EAAE,CAAC,EAAE,OAAO,CAAC,CACpC,CAAC;QACJ,CAAC;QAED,OAAO,OAAO,CAAC;IACjB,CAAC;IAED;;OAEG;IACH,KAAK,CAAC,IAAI,CAAC,MAAkB;QAC3B,MAAM,IAAI,CAAC,iBAAiB,EAAE,CAAC;QAE/B,MAAM,SAAS,GAAG,IAAI,CAAC,GAAG,EAAE,CAAC;QAC7B,MAAM,MAAM,GAAG,IAAI,CAAC,cAAc,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC;QAEpD,IAAI,CAAC;YACH,0CAA0C;YAC1C,MAAM,QAAQ,GAAG,IAAI,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC;YACrC,OAAO,CAAC,GAAG,CAAC,oBAAoB,QAAQ,CAAC,MAAM,EAAE,CAAC,CAAC;YAEnD,2CAA2C;YAC3C,IAAI,WAAW,GAAG,IAAI,CAAC,iBAAiB,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;YAE/C,6BAA6B;YAC7B,MAAM,WAAW,GAAG,CAAC,GAAG,QAAQ,CAAC,CAAC;YAClC,MAAM,SAAS,GAAa,EAAE,CAAC;YAE/B,8CAA8C;YAC9C,MAAM,SAAS,GAAG,QAAQ,CAAC,MAAM,GAAG,CAAC,MAAM,CAAC,SAAS,IAAI,IAAI,CAAC,MAAM,CAAC,SAAS,CAAC,CAAC;YAEhF,iCAAiC;YACjC,MAAM,YAAY,GAAG,MAAM,CAAC,SAAS,IAAI,IAAI,CAAC,MAAM,CAAC,SAAS,CAAC;YAE/D,KAAK,IAAI,IAAI,GAAG,CAAC,EAAE,IAAI,GAAG,YAAY,EAAE,IAAI,EAAE,EAAE,CAAC;gBAC/C,kFAAkF;gBAClF,MAAM,eAAe,GAAG,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,SAAS,CAAC,SAAS,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;gBAClF,MAAM,aAAa,GAAG,eAAe,CAAC,MAAM,CAAC;gBAE7C,uCAAuC;gBACvC,MAAM,WAAW,GAAG,IAAI,GAAG,CAAC,MAAM,CAChC,OAAO,EACP,aAAa,CAAC,IAAI,CAAC,eAAe,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,EAC/C,CAAC,CAAC,EAAE,aAAa,CAAC,CACnB,CAAC;gBAEF,yCAAyC;gBACzC,MAAM,WAAW,GAAG,WAAW,CAAC,MAAM,CAAC;gBACvC,MAAM,aAAa,GAAG,IAAI,GAAG,CAAC,MAAM,CAClC,OAAO,EACP,aAAa,CAAC,IAAI,CAAC,KAAK,CAAC,WAAW,CAAC,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC,EAC/C,CAAC,CAAC,EAAE,WAAW,CAAC,CACjB,CAAC;gBAEF,uDAAuD;gBACvD,MAAM,KAAK,GAA+B;oBACxC,SAAS,EAAE,WAAW;oBACtB,cAAc,EAAE,aAAa;oBAC7B,GAAG,WAAW;iBACf,CAAC;gBAEF,gBAAgB;gBAChB,MAAM,OAAO,GAAG,MAAM,IAAI,CAAC,OAAQ,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC;gBAE/C,4CAA4C;gBAC5C,MAAM,MAAM,GAAG,OAAO,CAAC,MAAM,CAAC,IAAoB,CAAC;gBACnD,MAAM,SAAS,GAAG,OAAO,CAAC,MAAM,CAAC,IAAI,CAAC,OAAO,CAAC,MAAM,CAAC,IAAI,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;gBAEtE,gCAAgC;gBAChC,MAAM,qBAAqB,GAAG,CAAC,aAAa,GAAG,CAAC,CAAC,GAAG,SAAS,CAAC;gBAE9D,uCAAuC;gBACvC,IAAI,SAAS,GAAG,CAAC,CAAC;gBAClB,IAAI,MAAM,GAAG,CAAC,QAAQ,CAAC;gBAEvB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,SAAS,EAAE,CAAC,EAAE,EAAE,CAAC;oBACnC,MAAM,KAAK,GAAG,MAAM,CAAC,qBAAqB,GAAG,CAAC,CAAC,GAAG,CAAC,MAAM,CAAC,WAAW,IAAI,IAAI,CAAC,MAAM,CAAC,WAAW,CAAC,CAAC;oBAClG,IAAI,KAAK,GAAG,MAAM,EAAE,CAAC;wBACnB,MAAM,GAAG,KAAK,CAAC;wBACf,SAAS,GAAG,CAAC,CAAC;oBAChB,CAAC;gBACH,CAAC;gBAED,gBAAgB;gBAChB,SAAS,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC;gBAC1B,WAAW,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC;gBAE5B,wDAAwD;gBACxD,IAAI,SAAS,KAAK,CAAC,IAAI,SAAS,KAAK,CAAC,EAAE,CAAC;oBACvC,OAAO,CAAC,GAAG,CAAC,2BAA2B,SAAS,EAAE,CAAC,CAAC;oBACpD,MAAM;gBACR,CAAC;gBAED,kDAAkD;gBAClD,WAAW,GAAG,EAAE,CAAC;gBACjB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,EAAE,EAAE,CAAC,EAAE,EAAE,CAAC;oBAC5B,WAAW,CAAC,mBAAmB,CAAC,MAAM,CAAC,GAAG,OAAO,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC;oBACtE,WAAW,CAAC,mBAAmB,CAAC,QAAQ,CAAC,GAAG,OAAO,CAAC,WAAW,CAAC,QAAQ,CAAC,CAAC;gBAC5E,CAAC;gBAED,qBAAqB;gBACrB,IAAI,CAAC,IAAI,GAAG,CAAC,CAAC,GAAG,EAAE,KAAK,CAAC,EAAE,CAAC;oBAC1B,OAAO,CAAC,GAAG,CAAC,gBAAgB,IAAI,GAAG,CAAC,YAAY,CAAC,CAAC;gBACpD,CAAC;YACH,CAAC;YAED,yCAAyC;YACzC,MAAM,aAAa,GAAG,IAAI,CAAC,MAAM,CAAC,SAAS,CAAC,CAAC;YAC7C,MAAM,OAAO,GAAG,IAAI,CAAC,GAAG,EAAE,GAAG,SAAS,CAAC;YACvC,MAAM,eAAe,GAAG,CAAC,SAAS,CAAC,MAAM,GAAG,CAAC,OAAO,GAAG,IAAI,CAAC,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;YAEzE,OAAO,CAAC,GAAG,CAAC,gBAAgB,aAAa,EAAE,CAAC,CAAC;YAC7C,OAAO,CAAC,GAAG,CAAC,gBAAgB,OAAO,OAAO,eAAe,cAAc,CAAC,CAAC;YAEzE,MAAM,OAAO,GAAmB,CAAC;oBAC/B,IAAI,EAAE,MAAM;oBACZ,IAAI,EAAE,aAAa,CAAC,IAAI,EAAE;iBAC3B,CAAC,CAAC;YAEH,OAAO;gBACL,EAAE,EAAE,cAAc,IAAI,CAAC,GAAG,EAAE,EAAE;gBAC9B,KAAK,EAAE,IAAI,CAAC,MAAM,CAAC,SAAS;gBAC5B,OAAO;gBACP,UAAU,EAAE,UAAU;gBACtB,KAAK,EAAE;oBACL,WAAW,EAAE,QAAQ,CAAC,MAAM;oBAC5B,YAAY,EAAE,SAAS,CAAC,MAAM;iBAC/B;gBACD,QAAQ,EAAE;oBACR,QAAQ,EAAE,YAAY;oBACtB,KAAK,EAAE,0BAA0B;oBACjC,OAAO;oBACP,IAAI,EAAE,CAAC,EAAE,0BAA0B;oBACnC,kBAAkB,EAAE,IAAI,CAAC,MAAM,CAAC,kBAAkB;oBAClD,eAAe,EAAE,UAAU,CAAC,eAAe,CAAC;iBAC7C;aACF,CAAC;QAEJ,CAAC;QAAC,OAAO,KAAK,EAAE,CAAC;YACf,MAAM,aAAa,GAAkB;gBACnC,IAAI,EAAE,oBAAoB;gBAC1B,OAAO,EAAE,0BAA0B,KAAK,EAAE;gBAC1C,QAAQ,EAAE,YAAY;gBACtB,SAAS,EAAE,IAAI;aAChB,CAAC;YACF,MAAM,aAAa,CAAC;QACtB,CAAC;IACH,CAAC;IAED;;OAEG;IACH,KAAK,CAAC,CAAC,MAAM,CAAC,MAAkB;QAC9B,MAAM,IAAI,KAAK,CAAC,wDAAwD,CAAC,CAAC;IAC5E,CAAC;IAED;;OAEG;IACH,oBAAoB,CAAC,QAAkB;QACrC,MAAM,SAAS,GAAG,CAAC,MAAM,CAAC,CAAC;QAC3B,OAAO,QAAQ,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,SAAS,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC;IACpD,CAAC;IAED;;OAEG;IACH,YAAY;QACV,OAAO;YACL,SAAS,EAAE,IAAI,CAAC,MAAM,CAAC,SAAS;YAChC,kBAAkB,EAAE,IAAI,CAAC,MAAM,CAAC,kBAAkB;YAClD,WAAW,EAAE,IAAI,CAAC,OAAO,KAAK,IAAI;YAClC,eAAe,EAAE,IAAI,CAAC,QAAQ,KAAK,IAAI;SACxC,CAAC;IACJ,CAAC;IAED;;OAEG;IACH,KAAK,CAAC,OAAO;QACX,IAAI,IAAI,CAAC,OAAO,EAAE,CAAC;YACjB,gEAAgE;YAChE,IAAI,CAAC,OAAO,GAAG,IAAI,CAAC;QACtB,CAAC;QACD,IAAI,IAAI,CAAC,QAAQ,EAAE,CAAC;YAClB,IAAI,CAAC,QAAQ,CAAC,IAAI,EAAE,CAAC;YACrB,IAAI,CAAC,QAAQ,GAAG,IAAI,CAAC;QACvB,CAAC;IACH,CAAC;CACF","sourcesContent":["/**\n * ONNX Runtime Local Inference Provider for Phi-4\n *\n * Uses onnxruntime-node for true local CPU/GPU inference\n */\n\nimport * as ort from 'onnxruntime-node';\nimport * as fs from 'fs';\nimport * as path from 'path';\nimport { get_encoding } from 'tiktoken';\nimport { ensurePhi4Model, ModelDownloader } from '../../utils/model-downloader.js';\nimport type {\n  LLMProvider,\n  ChatParams,\n  ChatResponse,\n  StreamChunk,\n  ProviderError,\n  Message,\n  ContentBlock\n} from '../types.js';\n\nexport interface ONNXLocalConfig {\n  modelPath?: string;\n  executionProviders?: string[];\n  maxTokens?: number;\n  temperature?: number;\n}\n\nexport class ONNXLocalProvider implements LLMProvider {\n  name = 'onnx-local';\n  type = 'custom' as const;\n  supportsStreaming = false; // Streaming requires complex token generation loop\n  supportsTools = false;\n  supportsMCP = false;\n\n  private session: ort.InferenceSession | null = null;\n  private config: Required<ONNXLocalConfig>;\n  private tokenizer: any = null;\n  private tiktoken: any = null;\n\n  constructor(config: ONNXLocalConfig = {}) {\n    this.config = {\n      modelPath: config.modelPath || './models/phi-4/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx',\n      executionProviders: config.executionProviders || ['cpu'],\n      maxTokens: config.maxTokens || 100,\n      temperature: config.temperature || 0.7\n    };\n  }\n\n  /**\n   * Load optimized tiktoken tokenizer (cl100k_base for Phi-4)\n   */\n  private async loadTokenizer(): Promise<void> {\n    if (this.tiktoken) return;\n\n    try {\n      // Use cl100k_base encoding (GPT-4, similar to Phi-4)\n      this.tiktoken = get_encoding('cl100k_base');\n\n      console.log('‚úÖ Tokenizer loaded (tiktoken cl100k_base)');\n    } catch (error) {\n      console.error('‚ùå Failed to load tiktoken:', error);\n      throw new Error(`Tokenizer loading failed: ${error}`);\n    }\n  }\n\n  /**\n   * Encode text using tiktoken (fast BPE)\n   */\n  private encode(text: string): number[] {\n    return Array.from(this.tiktoken.encode(text));\n  }\n\n  /**\n   * Decode tokens using tiktoken\n   */\n  private decode(ids: number[]): string {\n    try {\n      const decoded = this.tiktoken.decode(new Uint32Array(ids));\n      // tiktoken returns buffer, convert to string\n      if (typeof decoded === 'string') {\n        return decoded;\n      } else if (decoded instanceof Uint8Array || decoded instanceof Buffer) {\n        return new TextDecoder().decode(decoded);\n      }\n      return String(decoded);\n    } catch (error) {\n      console.warn('Decode error, returning raw IDs:', error);\n      return ids.join(',');\n    }\n  }\n\n  /**\n   * Initialize ONNX session (with automatic model download)\n   */\n  private async initializeSession(): Promise<void> {\n    if (this.session) return;\n\n    try {\n      // Ensure model is downloaded\n      console.log(`üîç Checking for Phi-4 ONNX model...`);\n\n      const modelPath = await ensurePhi4Model((progress) => {\n        if (progress.percentage % 10 < 1) { // Log every ~10%\n          console.log(`   üì• Downloading: ${ModelDownloader.formatProgress(progress)}`);\n        }\n      });\n\n      // Update config with actual model path\n      this.config.modelPath = modelPath;\n\n      console.log(`üì¶ Loading ONNX model: ${this.config.modelPath}`);\n\n      this.session = await ort.InferenceSession.create(\n        this.config.modelPath,\n        {\n          executionProviders: this.config.executionProviders as any,\n          graphOptimizationLevel: 'all',\n          enableCpuMemArena: true,\n          enableMemPattern: true\n        }\n      );\n\n      console.log(`‚úÖ ONNX model loaded`);\n      console.log(`üîß Execution providers: ${this.config.executionProviders.join(', ')}`);\n\n      // Load tokenizer\n      await this.loadTokenizer();\n\n    } catch (error) {\n      const providerError: ProviderError = {\n        name: 'ONNXInitError',\n        message: `Failed to initialize ONNX model: ${error}`,\n        provider: 'onnx-local',\n        retryable: false\n      };\n      throw providerError;\n    }\n  }\n\n  /**\n   * Format messages for Phi-4 chat template\n   */\n  private formatMessages(messages: Message[]): string {\n    let prompt = '';\n\n    for (const msg of messages) {\n      const content = typeof msg.content === 'string'\n        ? msg.content\n        : msg.content.map(c => c.type === 'text' ? c.text : '').join('');\n\n      if (msg.role === 'system') {\n        prompt += `<|system|>\\n${content}<|end|>\\n`;\n      } else if (msg.role === 'user') {\n        prompt += `<|user|>\\n${content}<|end|>\\n`;\n      } else if (msg.role === 'assistant') {\n        prompt += `<|assistant|>\\n${content}<|end|>\\n`;\n      }\n    }\n\n    prompt += '<|assistant|>\\n';\n    return prompt;\n  }\n\n  /**\n   * Initialize KV cache tensors for all 32 layers\n   * Phi-4 architecture: 32 layers, 8 KV heads, 128 head_dim\n   */\n  private initializeKVCache(batchSize: number, sequenceLength: number) {\n    const numLayers = 32;\n    const numKVHeads = 8;\n    const headDim = 128; // 3072 / 24 = 128\n    const kvCache: Record<string, ort.Tensor> = {};\n\n    // Initialize empty cache for each layer (key and value)\n    for (let i = 0; i < numLayers; i++) {\n      // Empty cache: [batch_size, num_kv_heads, 0, head_dim]\n      const emptyCache = new Float32Array(0);\n\n      kvCache[`past_key_values.${i}.key`] = new ort.Tensor(\n        'float32',\n        emptyCache,\n        [batchSize, numKVHeads, 0, headDim]\n      );\n\n      kvCache[`past_key_values.${i}.value`] = new ort.Tensor(\n        'float32',\n        emptyCache,\n        [batchSize, numKVHeads, 0, headDim]\n      );\n    }\n\n    return kvCache;\n  }\n\n  /**\n   * Chat completion using ONNX with KV cache\n   */\n  async chat(params: ChatParams): Promise<ChatResponse> {\n    await this.initializeSession();\n\n    const startTime = Date.now();\n    const prompt = this.formatMessages(params.messages);\n\n    try {\n      // Tokenize input using optimized tiktoken\n      const inputIds = this.encode(prompt);\n      console.log(`üìù Input tokens: ${inputIds.length}`);\n\n      // Initialize KV cache (reusable for batch)\n      let pastKVCache = this.initializeKVCache(1, 0);\n\n      // Track all generated tokens\n      const allTokenIds = [...inputIds];\n      const outputIds: number[] = [];\n\n      // Pre-allocate tensor buffers for performance\n      const maxSeqLen = inputIds.length + (params.maxTokens || this.config.maxTokens);\n\n      // Autoregressive generation loop\n      const maxNewTokens = params.maxTokens || this.config.maxTokens;\n\n      for (let step = 0; step < maxNewTokens; step++) {\n        // For first step, use all input tokens; for subsequent steps, use only last token\n        const currentInputIds = step === 0 ? inputIds : [outputIds[outputIds.length - 1]];\n        const currentSeqLen = currentInputIds.length;\n\n        // Create input tensor for current step\n        const inputTensor = new ort.Tensor(\n          'int64',\n          BigInt64Array.from(currentInputIds.map(BigInt)),\n          [1, currentSeqLen]\n        );\n\n        // Create attention mask for current step\n        const totalSeqLen = allTokenIds.length;\n        const attentionMask = new ort.Tensor(\n          'int64',\n          BigInt64Array.from(Array(totalSeqLen).fill(1n)),\n          [1, totalSeqLen]\n        );\n\n        // Build feeds with input, attention mask, and KV cache\n        const feeds: Record<string, ort.Tensor> = {\n          input_ids: inputTensor,\n          attention_mask: attentionMask,\n          ...pastKVCache\n        };\n\n        // Run inference\n        const results = await this.session!.run(feeds);\n\n        // Get logits for next token (last position)\n        const logits = results.logits.data as Float32Array;\n        const vocabSize = results.logits.dims[results.logits.dims.length - 1];\n\n        // Extract logits for last token\n        const lastTokenLogitsOffset = (currentSeqLen - 1) * vocabSize;\n\n        // Apply temperature and get next token\n        let nextToken = 0;\n        let maxVal = -Infinity;\n\n        for (let i = 0; i < vocabSize; i++) {\n          const logit = logits[lastTokenLogitsOffset + i] / (params.temperature || this.config.temperature);\n          if (logit > maxVal) {\n            maxVal = logit;\n            nextToken = i;\n          }\n        }\n\n        // Add to output\n        outputIds.push(nextToken);\n        allTokenIds.push(nextToken);\n\n        // Check for end token (2 is typical EOS for Phi models)\n        if (nextToken === 2 || nextToken === 0) {\n          console.log(`üõë Stop token detected: ${nextToken}`);\n          break;\n        }\n\n        // Update KV cache from outputs for next iteration\n        pastKVCache = {};\n        for (let i = 0; i < 32; i++) {\n          pastKVCache[`past_key_values.${i}.key`] = results[`present.${i}.key`];\n          pastKVCache[`past_key_values.${i}.value`] = results[`present.${i}.value`];\n        }\n\n        // Progress indicator\n        if ((step + 1) % 10 === 0) {\n          console.log(`üîÑ Generated ${step + 1} tokens...`);\n        }\n      }\n\n      // Decode output using optimized tiktoken\n      const generatedText = this.decode(outputIds);\n      const latency = Date.now() - startTime;\n      const tokensPerSecond = (outputIds.length / (latency / 1000)).toFixed(1);\n\n      console.log(`‚úÖ Generated: ${generatedText}`);\n      console.log(`‚è±Ô∏è  Latency: ${latency}ms (${tokensPerSecond} tokens/sec)`);\n\n      const content: ContentBlock[] = [{\n        type: 'text',\n        text: generatedText.trim()\n      }];\n\n      return {\n        id: `onnx-local-${Date.now()}`,\n        model: this.config.modelPath,\n        content,\n        stopReason: 'end_turn',\n        usage: {\n          inputTokens: inputIds.length,\n          outputTokens: outputIds.length\n        },\n        metadata: {\n          provider: 'onnx-local',\n          model: 'Phi-4-mini-instruct-onnx',\n          latency,\n          cost: 0, // Local inference is free\n          executionProviders: this.config.executionProviders,\n          tokensPerSecond: parseFloat(tokensPerSecond)\n        }\n      };\n\n    } catch (error) {\n      const providerError: ProviderError = {\n        name: 'ONNXInferenceError',\n        message: `ONNX inference failed: ${error}`,\n        provider: 'onnx-local',\n        retryable: true\n      };\n      throw providerError;\n    }\n  }\n\n  /**\n   * Streaming not implemented (requires complex generation loop)\n   */\n  async *stream(params: ChatParams): AsyncGenerator<StreamChunk> {\n    throw new Error('Streaming not yet implemented for ONNX local inference');\n  }\n\n  /**\n   * Validate capabilities\n   */\n  validateCapabilities(features: string[]): boolean {\n    const supported = ['chat'];\n    return features.every(f => supported.includes(f));\n  }\n\n  /**\n   * Get model info\n   */\n  getModelInfo() {\n    return {\n      modelPath: this.config.modelPath,\n      executionProviders: this.config.executionProviders,\n      initialized: this.session !== null,\n      tokenizerLoaded: this.tiktoken !== null\n    };\n  }\n\n  /**\n   * Cleanup resources\n   */\n  async dispose(): Promise<void> {\n    if (this.session) {\n      // ONNX Runtime sessions don't have explicit disposal in Node.js\n      this.session = null;\n    }\n    if (this.tiktoken) {\n      this.tiktoken.free();\n      this.tiktoken = null;\n    }\n  }\n}\n"]}
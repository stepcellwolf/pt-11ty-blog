{"version":3,"file":"types.js","sourceRoot":"","sources":["../../src/router/types.ts"],"names":[],"mappings":"AAAA,oCAAoC","sourcesContent":["// Core types for multi-model router\n\nexport interface LLMProvider {\n  name: string;\n  type: ProviderType;\n  supportsStreaming: boolean;\n  supportsTools: boolean;\n  supportsMCP: boolean;\n  chat(params: ChatParams): Promise<ChatResponse>;\n  stream?(params: ChatParams): AsyncGenerator<StreamChunk>;\n  validateCapabilities(features: string[]): boolean;\n}\n\nexport type ProviderType = 'anthropic' | 'openai' | 'openrouter' | 'ollama' | 'litellm' | 'onnx' | 'gemini' | 'custom';\n\nexport interface ChatParams {\n  model: string;\n  messages: Message[];\n  temperature?: number;\n  maxTokens?: number;\n  tools?: Tool[];\n  toolChoice?: 'auto' | 'any' | 'none' | { type: 'tool'; name: string };\n  stream?: boolean;\n  metadata?: Record<string, any>;\n  provider?: string; // Force specific provider (gemini, openrouter, anthropic, etc.)\n}\n\nexport interface Message {\n  role: 'user' | 'assistant' | 'system';\n  content: string | ContentBlock[];\n}\n\nexport interface ContentBlock {\n  type: 'text' | 'tool_use' | 'tool_result';\n  text?: string;\n  id?: string;\n  name?: string;\n  input?: any;\n  content?: any;\n  is_error?: boolean;\n}\n\nexport interface Tool {\n  name: string;\n  description: string;\n  input_schema: {\n    type: 'object';\n    properties: Record<string, any>;\n    required?: string[];\n  };\n}\n\nexport interface ChatResponse {\n  id: string;\n  model: string;\n  content: ContentBlock[];\n  stopReason?: 'end_turn' | 'max_tokens' | 'tool_use' | 'stop_sequence';\n  usage?: {\n    inputTokens: number;\n    outputTokens: number;\n  };\n  metadata?: {\n    provider: string;\n    model?: string;\n    cost?: number;\n    latency?: number;\n    executionProviders?: string[];\n    [key: string]: any;\n  };\n}\n\nexport interface StreamChunk {\n  type: 'content_block_start' | 'content_block_delta' | 'content_block_stop' | 'message_start' | 'message_delta' | 'message_stop';\n  delta?: {\n    type: 'text_delta' | 'input_json_delta';\n    text?: string;\n    partial_json?: string;\n  };\n  content_block?: ContentBlock;\n  message?: Partial<ChatResponse>;\n  usage?: {\n    inputTokens: number;\n    outputTokens: number;\n  };\n}\n\nexport interface ProviderConfig {\n  apiKey?: string;\n  baseUrl?: string;\n  organization?: string;\n  models?: {\n    default: string;\n    fast?: string;\n    advanced?: string;\n    [key: string]: string | undefined;\n  };\n  timeout?: number;\n  maxRetries?: number;\n  retryDelay?: number;\n  rateLimit?: {\n    requestsPerMinute?: number;\n    tokensPerMinute?: number;\n  };\n  preferences?: Record<string, any>;\n  // ONNX-specific config\n  modelPath?: string;\n  executionProviders?: string[];\n  maxTokens?: number;\n  temperature?: number;\n  localInference?: boolean;\n  gpuAcceleration?: boolean;\n}\n\nexport interface RouterConfig {\n  version: string;\n  defaultProvider: ProviderType;\n  fallbackChain?: ProviderType[];\n  providers: Record<ProviderType, ProviderConfig>;\n  routing?: RoutingConfig;\n  toolCalling?: ToolCallingConfig;\n  monitoring?: MonitoringConfig;\n  cache?: CacheConfig;\n}\n\nexport interface RoutingConfig {\n  mode: 'manual' | 'cost-optimized' | 'performance-optimized' | 'quality-optimized' | 'rule-based';\n  rules?: RoutingRule[];\n  costOptimization?: {\n    enabled: boolean;\n    maxCostPerRequest?: number;\n    budgetAlerts?: {\n      daily?: number;\n      monthly?: number;\n    };\n    preferCheaper?: boolean;\n    costThreshold?: number;\n  };\n  performance?: {\n    timeout?: number;\n    concurrentRequests?: number;\n    circuitBreaker?: {\n      enabled: boolean;\n      threshold: number;\n      timeout: number;\n      resetTimeout?: number;\n    };\n  };\n}\n\nexport interface RoutingRule {\n  condition: {\n    agentType?: string[];\n    requiresTools?: boolean;\n    complexity?: 'low' | 'medium' | 'high';\n    privacy?: 'low' | 'medium' | 'high';\n    localOnly?: boolean;\n    requiresReasoning?: boolean;\n  };\n  action: {\n    provider: ProviderType;\n    model: string;\n    temperature?: number;\n    maxTokens?: number;\n  };\n  reason?: string;\n}\n\nexport interface ToolCallingConfig {\n  translationEnabled: boolean;\n  defaultFormat: 'anthropic' | 'openai' | 'auto-detect';\n  formatMapping?: Record<string, string>;\n  fallbackStrategy?: 'disable-tools' | 'use-text' | 'fail';\n}\n\nexport interface MonitoringConfig {\n  enabled: boolean;\n  logLevel: 'debug' | 'info' | 'warn' | 'error';\n  metrics?: {\n    trackCost?: boolean;\n    trackLatency?: boolean;\n    trackTokens?: boolean;\n    trackErrors?: boolean;\n  };\n  alerts?: {\n    costThreshold?: number;\n    errorRate?: number;\n    latencyThreshold?: number;\n  };\n}\n\nexport interface CacheConfig {\n  enabled: boolean;\n  ttl?: number;\n  maxSize?: number;\n  strategy?: 'lru' | 'fifo' | 'lfu';\n  providers?: Record<string, { ttl?: number }>;\n}\n\nexport interface RouterMetrics {\n  totalRequests: number;\n  totalCost: number;\n  totalTokens: {\n    input: number;\n    output: number;\n  };\n  providerBreakdown: Record<string, {\n    requests: number;\n    cost: number;\n    avgLatency: number;\n    errors: number;\n  }>;\n  agentBreakdown?: Record<string, {\n    requests: number;\n    cost: number;\n  }>;\n}\n\nexport interface ProviderError extends Error {\n  provider: string;\n  statusCode?: number;\n  retryable: boolean;\n}\n"]}